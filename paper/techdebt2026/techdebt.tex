\documentclass[10pt,conference]{IEEEtran}
\usepackage{graphicx,url}
\usepackage{hyperref}
\usepackage{nicematrix}
\usepackage{array}
\usepackage[utf8]{inputenc}  
\usepackage{seqsplit}
\usepackage{todonotes}

%% Takeaway Boxes %%
\usepackage[most]{tcolorbox}
\tcbset{colback=gray!10, colframe=gray!50!black, boxrule=0.5pt,
        left=1mm, right=1mm, top=1mm, bottom=1mm, sharp corners}

\newenvironment{highlight-box}[1]{%
  \begin{tcolorbox}
  \textbf{#1:} \itshape}{\end{tcolorbox}}
%% Takeaway Boxes %%

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{When Do You Repeat Yourself?\\
Voices from the Trenches of Linux Kernel Maintainers on Code Duplication}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations

\author{\IEEEauthorblockN{Anonymous Authors}}

%\author{\IEEEauthorblockN{Luan Arcanjo, Marcelo Spessoto, David Tadokoro, Paulo Meirelles}
%\IEEEauthorblockA{
%Free Software Competence Center\\
%Institute of Mathematics, Statistics, and Computer Science
%Universidade de São Paulo, Brazil\\
%\{luanicaro,marcelomspessoto,davidbtadokoro\}@usp.br,paulormm@ime.usp.br}
%}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
The vast scale and continuous evolution of the Linux kernel make its maintenance a complex undertaking, where code duplication remains a persistent challenge that can hinder development and introduce bugs.
%
This paper presents a multimethod ethnographic study investigating the socio-technical dynamics of contributing to the Linux kernel by reducing identified code duplication.
%
To support this study, we developed a command-line utility tool for detecting function-level duplications, offering a concrete entry point for new contributors.
%
Our ethnographic approach, including complete participant observation and participant-as-observer analyses, demonstrated that addressing duplications is viable for lowering the contribution barrier. This point is evidenced by 8 of 13 (62\%) accepted contributions (patches) from 24 newcomers (16 undergraduate and 8 graduate students) in the Linux kernel project, collectively removing 1.397 lines of duplicated code.
%
Nevertheless, the study reveals a more intriguing and complex reality beyond mindlessly removing clones. An analysis of maintainer feedback on both accepted and rejected contributions highlights a nuanced understanding of technical debt due to code duplication within the Linux kernel community, where the benefits of eliminating duplications are carefully weighed against factors such as readability, the introduction of new abstractions, and the specific code context.
\end{abstract}

% no keywords

\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}

Hundreds or thousands of developers working together on a software product is a complex and demanding task. Similar or even exact copies are a frequent and often unavoidable outcome. Excessive duplication can negatively impact the maintainability of a project, as, when it occurs, any change to a repeated code segment (to fix a bug, for instance) requires replication across all other copies. Developers must, therefore, remain constantly vigilant to synchronize different parts of the codebase, which is a highly error-prone yet seemingly simple responsibility. Regarded as a major source of technical debt, removing or refactoring duplicated code demands great care to preserve the original behavior, with minor mistakes possibly leading to serious problems in the future.

A widespread dogma among developers is the \textit{Don't Repeat Yourself} principle~\cite{pragmatic-programmer}, stated as follows.

\begin{highlight-box}{DRY principle}
  ``We feel that the only way to develop software reliably, and to make our
  developments easier to understand and maintain, is to follow what we call the
  DRY principle: Every piece of knowledge must have a single, unambiguous,
  authoritative representation within a system''.
\end{highlight-box}

The Linux kernel is a foundational Free/Libre and Open Source Software (FLOSS) project, a vital component of the world's digital infrastructure. Considering only lines of code (no comments or blank lines) of strictly programming languages, in version 6.17, the main repository has more than 27 million lines of code and involved contributions from over twenty thousand developers. Within this context, code duplication remains a significant challenge,  reducing the code readability and increasing the risk of introducing bugs~\cite{harmone,harmtwo}. This issue is particularly prominent in kernel device drivers, which account for over 66\% of the Linux kernel source code.

The detection of code duplication, or \textit{code clones}, has been a research topic for decades~\cite{firstman}. The literature offers a widely accepted taxonomy that classifies clones into four types based on their similarity, from exact copies (Type-1) to semantically equivalent but syntactically different fragments (Type-4)~\cite{litreview}. Several detection techniques have emerged, including textual, token-based, tree-based, and graph-based approaches~\cite{litreview}. These efforts have culminated in state-of-the-art techniques, such as the graph-based approach proposed by Liu et al.~\cite{tailor}.

To address code duplication in the Linux kernel, we developed a command-line tool capable of detecting and analyzing function-level duplications\footnote{The tool and its documentation will be referenced after the review process.}. We conducted a multimethod ethnographic study leveraging the tool capabilities to identify duplications and submit contributions, directly or indirectly, to the Linux kernel, aiming to remove redundant code.
%
Through interactions with maintainers, this study gained insights into how the Linux community perceives the code quality related to code clones. Contrary to the DRY principle, our findings indicate that, in some contexts, maintainers can tolerate duplications for better readability, performance increases, or even to avoid integration overhead.
%\looseness=-1

Ethnography is a well-established qualitative method for understanding people, their cultures, and work practices~\cite{bookethno}. It provides insights into community members' values, beliefs, and practices~\cite{ethnosoft}. In this study, we first employed a complete participant observation approach~\cite{gold1958roles}, actively contributing by performing deduplications while documenting the process. We then conducted a participant-as-observer study~\cite{gold1958roles} with students resolving duplications identified by the tool. Beyond demonstrating that the tool lowered the entry barrier for new contributors, these studies yielded intriguing observations on kernel Linux deduplications. This paper describes the aforementioned studies, along with our visions and reflections from diving into the development process.

\section{The Linux Kernel}

The Linux kernel is a complex project organized into subsystems, such as the process scheduler, memory management, and device drivers. Each subsystem typically has a dedicated maintainer or a team of maintainers overseeing its development and managing contributions. The development process is coordinated through Git, and contributions are formatted as patches, text documents that outline the differences between two source code versions. These patches are then submitted to the relevant Linux mailing lists for public review and discussion.

This study focuses on two of these subsystems, selected to represent different contexts for contribution. We explore the \textit{AMD Display drivers}, chosen for their importance within the hardware ecosystem, which is responsible for natively enabling AMD GPU functionality in Linux. This subsystem is substantial, with over 391,000 lines of code across more than 1,089 files. We also investigate the \textit{Industrial I/O (IIO)} subsystem, often considered an accessible entry point for new contributors. The IIO subsystem provides support for devices that perform analog-to-digital or digital-to-analog conversions\footnote{\href{https://kernel.org/doc/html/v6.17/driver-api/iio/intro.html}{https://kernel.org/doc/html/v6.17/driver-api/iio/intro.html}} and contains over 281,772 lines of code in more than 755 files.

\section{Research Strategy}

We employed a multi-method research approach. We developed and evaluated a proposed tool using a method from the literature~\cite{bigclonebench}. Given this validation, we proceeded to understand the viability of reducing duplications found by the tool with an ethnographic study using a participant observation methodology~\cite{gold1958roles}. In the first phase, we interacted with the Linux kernel community and got their feedback (\textit{complete participant observation}~\cite{gold1958roles}) through sending deduplication patches to the AMD Display driver. In the second phase, during a university course, we proposed a similar activity of sending patches to AMD Display or IIO to students. At the same time, we guided and mentored them throughout the process (\textit{participant-as-observer}~\cite{gold1958roles}).

\subsection{Tool}

Academic research on code duplication often focuses on pairwise comparison of code artifacts, which is not directly applicable to comprehensive codebase analyses. Existing free software tools we explored in practice suffered from limited functionality~[omitted]. Thus, we developed a command-line tool\footnote{Anonymous repository:  \url{https://anonymous.4open.science/r/arkanjo-C548}} designed to detect and analyze function-level code duplication within large codebases, such as the Linux kernel. The tool is released under the LGPLv3 ( GNU Lesser General Public License, version 3) and employs a two-stage architecture that separates the computationally intensive task of finding duplicates from the process of querying the results. The first stage analyzes the entire codebase, parses C functions, and stores identified duplicates in a database. The second stage uses this database to retrieve information about the identified duplications quickly.
%The tool is available at \textit{\href{https://github.com/arkanjo-tool/arkanjo}{github.com/arkanjo-tool/arkanjo}}.

The tool was designed to address the specific challenges posed by large-scale codebases, focusing on scalability and prioritization of duplications that may impact maintainability. It uses a text similarity method based on TF-IDF vector embedding, implemented with the Gensim library~\cite{gensim}, and computes cosine similarity to detect clones.

We compared our tool against the BigCloneBench dataset~\cite{bigclonebench}. This validation showed that the tool is capable of detecting the majority of Type-1 and Type-2 duplications (100\% of Type-1 and 85\% of Type-2 with a cosine similarity threshold of 0.9), demonstrating sufficient accuracy to support our ethnographic studies.

\subsection{Complete Participant Observation Study}
\label{sec:participant}

To investigate whether we could use the tool to find ways to contribute to the Linux kernel, we conducted a complete participant observation experiment, where we acted as first-time contributors to the AMD Display driver, collecting artifacts of our experience in the process.

We executed the tool in the AMD Display driver codebase. We manually analyzed the biggest duplications regarding the number of lines, finding one pair that we judged promising to try to deduplicate. We approached big duplications as proof of concept to gauge the tool's potential for enabling significant impacts.

Given the duplicated function pair, we observed that the files in the functions contained multiple other duplications. Thus, we proposed a simple systematic approach to mitigate all the functions duplicated in the context, not just the duplicated function pairs. After approaching the duplications with a systematic strategy, we sent a patch to the AMD development mailing list for the feedback of maintainers, while documenting the process, interactions, and impressions.

In C programming, communication between source files is achieved by creating header files that specify libraries~\cite{Cbook}. Since the Linux kernel is primarily written in C, our systematic strategy consisted of eliminating code duplications through consolidating the duplicated code into a single library, which would replace instances of duplication across the codebase. For each duplicated function pair, for each function in the pair, we used the tool to locate all duplications of the function in the codebase, as it could have been duplicated more times than the two occurrences detected initially. This strategy resulted in a collection of duplicated functions and corresponding code files.

To identify shared code more effectively, we extended this approach to search for other common functions across all collection files. We then applied specific refactoring methods to each shared function. If the functions were identical across files, we removed the duplicates and created a single function in the library. If modifications existed, we applied case-specific refactoring.

\subsection{Participant-as-Observer Observation Study}

A core dynamic of the university course where we conducted the participant-as-observer study was to have students make a practical contribution to a free software project. In the first semester of 2025, we identified an opportunity to leverage the tool within this pedagogical structure. As a contribution option, we proposed that students remove code duplications found in the Linux kernel as one possible way of contributing to a free software project.

The 2025 course offering had 37 students (25 undergraduate and 12 graduate), who were asked to form groups of two or three members. To assist students in this task, we prepared multiple alternatives for contributing to free software projects, including simpler contributions such as updating documentation or fixing code style issues. In this context, two options related to removing duplications in the Linux kernel were offered.

In the first option, we ran the tool on the IIO subsystem, creating a list of duplicated function pairs. Then, we curated the list even more to get more actionable duplications for the students using this approach:

\begin{enumerate}
    \item We filtered the list to keep only duplicated pairs that happened in the same file, because the IIO subsystem is very decentralized, and a duplication between two different drivers may involve the cooperation of different maintainers and/or the unification of totally different interfaces;
    \item We removed duplications that were too short and would not represent a significant contribution;
    \item We manually ranked code duplications and marked some duplications with insights based on personal opinion and perceptions over the remaining entries. This was done to guide students so that they could approach the technical debt with a greater chance of having their contribution merged into the code.
\end{enumerate}

The students' task was to select a recommended entry from the list, refactor the code to eliminate these duplications, and then submit their patches to the IIO mailing list for the maintainers' review.

For the second alternative, students were offered an experience similar to our initial complete participant observation. In this model, students were responsible for the entire workflow: executing the tool on the AMD Display drivers, independently analyzing the results to identify a potential deduplication, creating a patch with it, and sending the patch to the driver's maintainers.

Of the students who chose to work on the tool-related tasks, 23 (16 undergraduate and 7 graduate) formed 11 groups to pursue the first alternative. One graduate student opted for the second alternative. Notably, none of these students had prior experience contributing to the Linux kernel, making this their first attempt to submit a patch as newcomers to the project.

Using blog posts, the groups documented their experiences and approaches to refactoring the duplications, as well as their experiences submitting a patch to the Linux kernel. We also analyzed these posts to understand the refactoring patterns used in deduplications, the experience of sending patches, and the opinions of subsystem maintainers on them.

\section{Results}

In the \textbf{complete participant observation} study, we chose one function pair found on AMD Display Driver to approach and apply the systematic approach.

We reached a point where we had an idea of how to remove the duplications found, but we could not complete the refactoring to a state capable of submitting as a patch to the driver. We found that the configuration files on the AMD Display driver are designed to be imported at compilation time using \textit{\#define} macros. The configuration files' design choice makes refactoring duplications on generic approaches trickier, as refactoring code that depends on these files requires significant modification of the configuration files' design and deep knowledge of the codebase, which we, as first-time contributors, did not have. Thus, we opted not to continue investigating this deduplication. We found that the duplicated function from the code reduction existed in five code files, and two other functions were duplicated in those five. All functions across the files were exactly equal, so there was no need to apply the refactoring methods presented in the literature. The refactoring was resumed to create a generic library and fix the compilation targets.

In this attempt, we reached a point where the deduplication was good enough to submit it as a contribution to the AMD Display driver code quality. Thus, we moved to sending the refactoring to the driver maintainers as a patch.

In the feedback on the first submitted version, the maintainers only asked for minor changes to align with the coding style, correct license use, and align with initially unknown conventions. We submitted a second version, in which the maintainers asked us to move the functions created in the generic library to an existing file instead of creating a new one. We sent a third version, and the patch was accepted and integrated into the kernel codebase on February 25, 2025.
%\footnote{Patch submitted: \href{https://lore.kernel.org/all/20250225015532.303032-1-luanicaro@usp.br/}{lore.kernel.org/all/20250225015532.303032-1-luanicaro@usp.br}}.

\begin{highlight-box}{Takeaway T1:}
  \textbf{As the primary result of our complete participant observation we
  understood that AMD Display driver developers do not necessarily view
  duplicated code negatively, which contradicts standard software engineering
  practices}. We suffered a delay in the review process, and to understand why,
  we directly contacted a maintainer. It was mentioned to us that one reason for
  the delay was that duplicated code enhances the independence of GPU driver
  code. This allows developers to make changes to a specific GPU without needing
  to test compatibility with others, which helps save a significant amount of
  time and effort. This example of hardware variations, also mentioned by Kapser
  and Godfrey~\cite{cloneharm}, corroborates our understanding that GPU driver
  developers do not view duplicated code negatively by default.
\end{highlight-box}

Regarding our analysis from the \textbf{participant-as-observer} phase, Table~\ref{tab:stu} summarizes all the contributions sent by newcomer groups (GID)~\footnote{As in the first phase, we were also newcomers, we included our experience in the table as Group 0. Nevertheless, the subsequent analysis will focus more on the patches sent by students.}. It contains the information about the chosen subsystem (SS) and the similarity threshold (SML). Each entry also has the patch status, takeaways that may have surfaced from the interaction, and a code differential corresponding to the final patch versions before it was dropped or accepted into the code base. The code differential for a set of patches corresponds to the sum of added and removed lines of each patch.

\begin{table}[ht]
\centering
\caption{Summary of newcomer deduplication contributions.}
\begin{tabular}{ |c |c |c |c |c | c| }
\hline
\textbf{GID} & \textbf{SS} & \textbf{SML} & \textbf{Patch Status} & \textbf{Takeaway} & \textbf{Diff} \\
\hline
0 & AMD & 100\% & Accepted (v3) & T1 & +114/-520 \\ \hline
1 & IIO & 100\% & Dropped (v2) & T2, T3 & +7/-14 \\ \hline
2 & IIO & 100\% & Dropped (v1) & T2 & +23/-27 \\ \hline
3 & IIO & 100\% & Accepted (v1) & None & +12/-30 \\ \hline
4 & IIO & 100\% & Accepted (v3) & T2 & +2/-7 \\ \hline
5 & IIO & 90\% & Accepted (v1) & None & +17/-70 \\ \hline
6 & IIO & 90\% & Dropped (v1) & T4 & +95/-92 \\ \hline
7 & IIO & 90\% & Accepted (v3) & None & +20/-36 \\ \hline
8 & IIO & 90\% & Accepted (v4) & T5 & +20/-16 \\ \hline
9 & IIO & 90\% & Accepted (v6) & T5 & +149/-243 \\ \hline
10 & IIO & 90\% & Dropped (v1) & T2, T5 & +32/-58 \\ \hline
11 & IIO & 100\% & Dropped (v1) & None & +2/-12 \\
   & AMD & 90\% & Accepted (v2) & None & +90/-489 \\ \hline

\end{tabular}%

\label{tab:stu}
\end{table}

We observed that most groups applied the parameterized refactor method while a few used the extractor method.

Groups 1 and 4 worked on similar duplications, where two functions are involved: one to determine if a register was volatile and the other to determine if it was writable. These were two functions that returned opposite Boolean results. Group 1 initially addressed the problem using the extract method, while Group 2 used macros. After feedback from maintainers, both solutions converged to keep one of the functions while changing the content of the other to return a negated query of the former function. However, by the end of the feedback cycle, maintainers considered the patch from Group 1 irrelevant, while the patch from Group 4 was accepted. The reason is that Group 1 chose to apply the refactoring to a device that contained a register that was both read-only and non-volatile. Thus, the technical debt selected by Group 1 was not merely a duplication but an incorrect implementation of the method.

Groups 3, 6, and 10 created a struct to encapsulate the parameters and remove the duplication by applying the parameterized method. The patches from Groups 6 and 10 were rejected by the same maintainer who approved the patch from Group 3. The maintainer's justification for rejecting the patch from Group 6 was that it proposed to fix a duplication while adding more lines than removing, and that it was not convincing enough in other aspects, such as readability. The reason for rejecting the patch from Group 10 was that they refactored code files with simple context; therefore, adding a layer of abstraction to remove duplication decreased code readability without clear gains in codebase quality. Interestingly, both rejected groups addressed duplications across initialization functions for different devices, where code readability was vulnerable to refactoring methods. In contrast, Group 3 tackled the duplication of a simple iterative algorithm, which also contained a bug that could be inferred by inspecting both functions. In this case, the group refactored code from a more critical technical debt and took the opportunity to fix an unnoticed incorrect behavior.

Groups 7 and 8 used the extract method to approach the deduplications. Group 8 sent their patch to the maintainers, and the code changes were approved, with minor requested changes in code style and smaller errors fixed. In the case of Group 7, the maintainer initially rejected the proposed patch but suggested a new refactoring approach: applying the extraction method refactoring to the duplicated code to an existing helper function. After the feedback cycle, maintainers accepted the patch in version 3.

Group 9 initially approached the selected duplication with a parameterized method, but after entering the feedback cycle, it was revealed that the problem represented a much more complex technical debt. From the first version to the last (version six), the patch was split into a series with four patches, where two of them have been applied, and the remaining entries are receiving suggestions. Even though the patch became a series of four, only one of them (one of the accepted) was strictly tackling duplication.

Group 11 was a single student who took the second option and executed the tool, completing the entire process of identifying duplications himself. The student identified a duplication in the driver that occurred across eighteen code files, creating a patch to remove approximately four hundred lines of duplicated code. The student sent the deduplications to the maintainers, and their feedback pointed to fixing warnings in the code without arguing about the merit of the duplication removals. After addressing the issue, the patch was applied. This student also contributed to IIO, tackling a code duplication with inline method refactoring. This one, however, was received with more skepticism by the maintainer, and the student dropped the contribution because he did not think the feedback instructions were clear enough.

Analyzing the students' approaches to the duplications and the maintainers' feedback, we saw that people without previous experience in Linux kernel development could approach the duplications found by the tool as a comprehensive and more straightforward way to become contributors to the kernel. We observed that not all duplications are viewed as code of bad quality, with maintainers analyzing the trade-off between the purpose of the duplicated code and many other factors.

The eleven groups, which have submitted 11 contributions to the IIO subsystem, have collectively requested the insertion of 379 lines and the deletion of 605 lines in the subsystem, representing an average of 34.45 lines added and 55 lines removed per contribution.

Considering the sum of fully merged contributions, we have a code diff value of +51/-143, with an average of +12.75/-35.75. This ratio is better than dropped contributions, which have a total of +179/-219 with an average of +29.83/-36.5.

It is also possible to notice a difference based on the similarity threshold used to detect duplications. The proposed patches to address 100\%-similarity duplications summed up to +46/-90 (average of +9.2/-18). For a 90\% value, the result was a total of +333/-515 with an average of +55.5/-85.83. The 90\% value enabled the tool to identify less strict duplication cases, and, therefore, code segments with a greater extension, compared to the 100\% similarity entries, which were shorter segments, causing a difference in the average number of modified lines.

Finally, the AMD contribution from Group 11, which involved the complete use of our developed tool components, got a completely different outcome. With a total of 90 lines inserted and 489 lines removed, there are some points to be considered for the success and high modification extension of the contribution: (1) the AMD Display subsystem has some self-reported problems regarding overall technical debt; (2) the AMD Display subsystem has a more centralized code structure and maintainer hierarchy, enabling contributors to tackle code duplications across a broader scope and multiple files, which happened to Group 11's contribution;

We derived the following takeaways from \textbf{participant-as-observer observation}. These takeaways are accompanied by example quotes from the maintainers that originated during the review process~\footnote{It is paramount to state that we \textbf{are not affirming} that these quotes represent the complete positioning of the maintainers who wrote them on code duplication.}.

\begin{highlight-box}{Takeaway T2 (Readability)}
  Maintainers notably prioritized readability over removing duplications in
  many different contexts. In their view, going against the \textbf{DRY
  principle} was worth it if it resulted in code that is easier to read and
  understand.
\hfill\\

\noindent
\begin{footnotesize}
\textit{
  Quote: ``I think the old code is more readable than hiding the values in a macro
  even if it is duplicating a few lines of code.''
}
\end{footnotesize}
\end{highlight-box}


\begin{highlight-box}{Takeaway T3 (Integration Overhead)}
  A maintainer suggested that the effort put into the contribution after it was
  accepted (i.e., the process of propagating the change to upper maintainers)
  was considered. In other words, maintainers may \textbf{evaluate the
  trade-off between the impact of contributions (not only deduplications) and
  the struggle to integrate them into the complex Linux project}.
\hfill\\

\noindent
\begin{footnotesize}
\textit{
  Quote: ``[...] such patches might not worth it since the proposed
  improvement is very small (and questionable) while the upstreaming process
  still requires some effort.''
}
\end{footnotesize}
\end{highlight-box}

%TODO Na citação abaixo, trazida pelo Arthur, pensar se não vale colocar as
%referências base do paper do Lanza: ``Developers need to mentally reconstruct
%the role and behavior of a class from scattered fragments of code [8],
%presented as one page of text at a time. This process is cognitively demanding
%[9], slow, and error-prone, especially in large systems with many
%interdependent components [10].''
%
% Referências do paper do Lanza para esse trecho:
%[8] M.-A. Storey, “Theories, methods and tools in program comprehension:
%Past, present and future,” in Proceedings of IWPC 2005. IEEE, 2005,
%pp. 181–191.
%[9] J. Sillito, G. C. Murphy, and K. De Volder, “Questions programmers
%ask during software evolution tasks,” in Proceedings FSE 2006. ACM,
%2006, pp. 23––34.
%[10] G. C. Murphy, D. Notkin, and K. Sullivan, “Software reflexion models:
%Bridging the gap between source and high-level models,” in Proceedings
%of FSE 1995. ACM, 1995, pp. 18–28.
\begin{highlight-box}{Takeaway T4 (Cognitive Load)}
  Duplications that made it possible to read the code without needing to jump
  to the function definition (to see its behavior) or the function call (to see
  its concrete parameters) were justifiable to reduce cognitive load when
  parsing through the code~\cite{skylines-cognitive-load}.
\hfill\\

\noindent
\begin{footnotesize}
\textit{
  Quote: ``Wrapping this up doesn't provide any real advantage, requiring as
  it does the reviewer to look at this function AND where the value is set
  rather than seeing them in one place.''
}
\end{footnotesize}
\end{highlight-box}


\begin{highlight-box}{Takeaway T5 (Performance)}
  Given the low-level development context of Linux and the IIO subsystem, which
  is usually related to devices with limited resources, we detected hints that
  duplication may be forgivable if it improves performance.
\hfill\\

\noindent
\begin{footnotesize}
\textit{
  Quote: ``Note that is not an appropriate change for the large reads though
  as \texttt{spi\_write\_then\_read()} bounces all buffers and so would add a
  copy to those high(ish) performance paths.''
}
\end{footnotesize}
\end{highlight-box}

%TODO ADICIONADO APENAS PARA FECHAR 5 PGS CERTINHO. ME REMOVA!!!
\hfill\\

\section{Concluding Remarks}

Ethnographic studies, including full participant and participant-as-observer observations, provided a realistic view of the opportunities and challenges in removing code duplications detected by the tool. First-time Linux kernel contributors could effectively use the tool to identify duplications and submit patches.

While this research included one successfully merged patch by the authors, and 7 out of 12 patches sent by students were accepted and merged into Linux, a portion of the students' efforts encountered hurdles: 4 student groups had their patches rejected.

Nevertheless, our work suggests that maintainers do not always view code duplication as inherently unfavorable, especially when it improves clarity, performance, and more. This suggests a nuanced understanding of code quality within the Linux kernel community that can go against the DRY principle, where practicality and maintainability in specific contexts sometimes outweigh the general principle of eliminating all code duplication.

%TODO ADICIONADO APENAS PARA FECHAR 5 PGS CERTINHO. ME REMOVA!!!
\hfill\\

\section*{Replication Package}

All anonymized threads containing the entire review process and interactions with maintainers are available in this Zenodo repository: \href{https://doi.org/10.5281/zenodo.17432193}{https://doi.org/10.5281/zenodo.17432193}.

To avoid violating the double-anonymized review process, the anonymous repository of the developed tool is available at \url{https://anonymous.4open.science/r/arkanjo-C548}. The tool website link and omitted references will be added in the camera-ready version of the paper.

\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,references}

\end{document}


